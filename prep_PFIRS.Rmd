---
title: "prep_PFIRS"
output: 
  html_document:
    toc: TRUE
date: "2023-09-05"
---

```{r, include = F}
require(tidyverse)
require(sf)
require(readxl)
require(mapview)
```

# Set params
```{r}
#file <- paste0(ref_path, "/PFIRS/PFIRS 2019-2020 CT pulled 2022.xlsx")
file <- paste0(ref_path, "/PFIRS/PFIRS 2017-2022 pulled 2023.xlsx")
pulled_year <- substr(file, nchar(file)-8, nchar(file)-5)
filter_year <- 2019
min_year <-  2017
max_year <-  2022
include_NF <- "no"
```

```{r}
print(paste("data was pulled in", pulled_year))
```

# Read in data
```{r}
xy <- read_excel(file)
xy %>% head()
```

# Clean

## Fix column names
```{r}
xy <- xy %>% 
  rename(Burn_Date = `Burn Date`) %>% 
  rename(Burn_Unit = `Burn Unit`) %>% 
  rename(Acres_Burned = `Acres Burned`) %>% 
  rename(Fuel_Type = `Fuel Type`) %>% 
  rename(Total_Tons = `Total Tons`) %>% 
  rename(Burn_Type = `Burn Type`) 
```

## Make lat and long numeric and remove unk rows
```{r}
xy <- xy %>% 
  filter(!Longitude == "UNK") %>% 
  mutate(Longitude = as.numeric(Longitude)) %>% 
  mutate(Latitude = as.numeric(Latitude))
```

## Fix dates that should be negative
```{r}
xy <- xy %>% 
  mutate(Longitude = ifelse(Longitude > 0, Longitude*(-1), Longitude))
```

## Add year column
```{r}
xy <- xy %>% 
  mutate(Year = year(Burn_Date))
```


```{r}
summary(as.factor(xy$Year))
```

## Remove rows with NA dates or dates outside the date range 
```{r}
nrow_old <- nrow(xy)
xy <- xy %>% 
  filter(Year <= max_year & Year >= min_year)
deleted_n <- nrow_old - nrow(xy)
print(paste("Deleted", deleted_n, "duplicated rows"))
```

## Delete duplicates

From Jason Branz: "Typically if there are multiple records with the same date, burn name, and acres, itâ€™s a duplicate." 

### Look at duplicates
```{r}
xy %>% 
  select(Year, Burn_Date, Burn_Unit, Acres_Burned) %>% 
  filter(duplicated(.))
```

### Remove duplicates
```{r}
nrow_old <- nrow(xy)
xy <- xy %>% 
  distinct(Burn_Date, Burn_Unit, Acres_Burned, .keep_all = T)
deleted_n <- nrow_old - nrow(xy)
print(paste("Deleted", deleted_n, "duplicated rows"))
```

# Make spatial: All years, all places

## Make spatial
```{r}
sf <- st_as_sf(xy, coords = c("Longitude", "Latitude"), crs = 4326, remove = F)
```

```{r}
mapview(sf)
```


### Export to shapefile
```{r, warning=F}
layer_name = paste0("PFIRS_", min_year, "-", max_year, "_pull", pulled_year)
st_write(sf, "shapefiles", layer_name, driver = "ESRI Shapefile", delete_layer = T)
```

### Save as Rdata
```{r}
pfirs <- sf
save(pfirs, file = paste0("Rdata/", layer_name, ".Rdata"))
remove(layer_name)
```


# Mask out federal lands

## Crop to state of CA
```{r}
CA <- st_read(dsn = paste0(ref_path, "/CA boundary/ca-state-boundary/", layer = "CA_State_TIGER2016.shp"))
CA <- st_transform(CA, st_crs(sf))
```

```{r}
sf <- st_intersection(sf, CA)
```


```{r}
load(file = "Rdata/NF.Rdata")
```

```{r}
NF <- st_transform(NF, st_crs(sf))
```

This prevents an error in geometry handling:
```{r}
sf_use_s2(FALSE)
```

```{r}
sf_noNF <- st_difference(sf, NF)
```

```{r}
mapview::mapview(list(NF, sf_noNF), col.regions=list("red","blue"),col=list("red","blue"))
```

#### Save as Rdata
```{r}
layer = paste0("Rdata/PFIRS_", min_year, "_", max_year, "_pull", pulled_year, "_noNF.Rdata" )
save(sf_noNF, file = "Rdata/PFIRS_2017-2022_pull2023_noNF.Rdata")
```

# Filter to `filter_year`s
```{r}
xy_year <- xy %>% 
  filter(Year == filter_year)
```

```{r}
xy_year %>% head()
```

## Make spatial
```{r}
sf_year <- st_as_sf(xy_year, coords = c("Longitude", "Latitude"), crs = 4326)
```

## Export to shapefile
```{r, warning = F}
layer_name = paste0("PFIRS_", filter_year, "_pull", pulled_year)
st_write(sf_year, dsn = "shapefiles", layer = layer_name, driver = "ESRI Shapefile", delete_layer = T)
```

## Write to .Rdata
```{r}
file = paste0("Rdata/PFIRS_", filter_year, "_pull", pulled_year, ".Rdata" )
save(sf_year, file = layer)
```



